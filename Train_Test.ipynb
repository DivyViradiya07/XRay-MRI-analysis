{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train-Test Split Complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define paths\n",
    "input_dir = \"Preprocessed_Images\"\n",
    "train_dir = \"Dataset/Train\"\n",
    "test_dir = \"Dataset/Test\"\n",
    "\n",
    "# Create train/test directories\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# Loop through each disease category\n",
    "for disease in os.listdir(input_dir):\n",
    "    disease_path = os.path.join(input_dir, disease)\n",
    "    images = os.listdir(disease_path)\n",
    "    \n",
    "    # Split into train (80%) and test (20%)\n",
    "    train_images, test_images = train_test_split(images, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create disease subfolders in train & test directories\n",
    "    os.makedirs(os.path.join(train_dir, disease), exist_ok=True)\n",
    "    os.makedirs(os.path.join(test_dir, disease), exist_ok=True)\n",
    "\n",
    "    # Move files to respective folders\n",
    "    for img in train_images:\n",
    "        shutil.copy(os.path.join(disease_path, img), os.path.join(train_dir, disease, img))\n",
    "\n",
    "    for img in test_images:\n",
    "        shutil.copy(os.path.join(disease_path, img), os.path.join(test_dir, disease, img))\n",
    "\n",
    "print(\"✅ Train-Test Split Complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set:\n",
      "Atelectasis: 368 images\n",
      "Cardiomegaly: 156 images\n",
      "Consolidation: 164 images\n",
      "Edema: 72 images\n",
      "Effusion: 389 images\n",
      "Emphysema: 100 images\n",
      "Fibrosis: 137 images\n",
      "Hernia: 21 images\n",
      "Infiltration: 664 images\n",
      "Mass: 128 images\n",
      "No Finding: 2203 images\n",
      "Nodule: 171 images\n",
      "Pleural_Thickening: 132 images\n",
      "Pneumonia: 52 images\n",
      "Pneumothorax: 159 images\n",
      "\n",
      "Test Set:\n",
      "Atelectasis: 92 images\n",
      "Cardiomegaly: 40 images\n",
      "Consolidation: 41 images\n",
      "Edema: 18 images\n",
      "Effusion: 98 images\n",
      "Emphysema: 25 images\n",
      "Fibrosis: 35 images\n",
      "Hernia: 6 images\n",
      "Infiltration: 166 images\n",
      "Mass: 33 images\n",
      "No Finding: 551 images\n",
      "Nodule: 43 images\n",
      "Pleural_Thickening: 33 images\n",
      "Pneumonia: 13 images\n",
      "Pneumothorax: 40 images\n"
     ]
    }
   ],
   "source": [
    "# Count files in Train and Test sets\n",
    "print(\"Train Set:\")\n",
    "for disease in os.listdir(train_dir):\n",
    "    print(f\"{disease}: {len(os.listdir(os.path.join(train_dir, disease)))} images\")\n",
    "\n",
    "print(\"\\nTest Set:\")\n",
    "for disease in os.listdir(test_dir):\n",
    "    print(f\"{disease}: {len(os.listdir(os.path.join(test_dir, disease)))} images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training CNN model using Pytorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step1: Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset Loaded Successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Define dataset class\n",
    "class XRayDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = os.listdir(root_dir)\n",
    "        self.files = []\n",
    "        \n",
    "        for label in self.classes:\n",
    "            class_path = os.path.join(root_dir, label)\n",
    "            for file in os.listdir(class_path):\n",
    "                self.files.append((os.path.join(class_path, file), self.classes.index(label)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.files[idx]\n",
    "        image = np.load(img_path)  # Load .npy image\n",
    "        image = torch.tensor(image, dtype=torch.float32).unsqueeze(0)  # Add channel dimension\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Define transformations (Normalization)\n",
    "transform = transforms.Normalize([0.5], [0.5])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = XRayDataset(\"Dataset/Train\", transform=transform)\n",
    "test_dataset = XRayDataset(\"Dataset/Test\", transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(\"✅ Dataset Loaded Successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step2: Setting up GPU for Training process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "GPU Name: NVIDIA GeForce MX550\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"GPU Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step3:Defining CNN model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CNN Model Defined & Moved to GPU!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define CNN Model\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(128 * 28 * 28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "num_classes = len(os.listdir(\"Dataset/Train\"))  # Number of disease categories\n",
    "model = CNNModel(num_classes).to(device)\n",
    "\n",
    "print(\"✅ CNN Model Defined & Moved to GPU!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step4: Using CrossEntropyLoss and AdamOptimiser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/154 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]: 100%|██████████| 154/154 [20:32<00:00,  8.00s/it, gpu_mem=0.84 GB, loss=1.79]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Avg Loss: 4.0140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/10]: 100%|██████████| 154/154 [18:33<00:00,  7.23s/it, gpu_mem=0.84 GB, loss=2.34]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Avg Loss: 2.1656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/10]: 100%|██████████| 154/154 [18:33<00:00,  7.23s/it, gpu_mem=0.84 GB, loss=2.07]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Avg Loss: 2.1170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/10]: 100%|██████████| 154/154 [18:32<00:00,  7.23s/it, gpu_mem=0.84 GB, loss=1.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Avg Loss: 2.0678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/10]: 100%|██████████| 154/154 [18:32<00:00,  7.22s/it, gpu_mem=0.84 GB, loss=2.07]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Avg Loss: 2.0542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6/10]: 100%|██████████| 154/154 [18:34<00:00,  7.24s/it, gpu_mem=0.84 GB, loss=1.94]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Avg Loss: 2.0658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [7/10]: 100%|██████████| 154/154 [18:38<00:00,  7.26s/it, gpu_mem=0.84 GB, loss=2.06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Avg Loss: 2.0270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [8/10]: 100%|██████████| 154/154 [18:38<00:00,  7.26s/it, gpu_mem=0.84 GB, loss=1.88]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Avg Loss: 1.9915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [9/10]: 100%|██████████| 154/154 [18:41<00:00,  7.28s/it, gpu_mem=0.84 GB, loss=2.04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Avg Loss: 1.9897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/10]: 100%|██████████| 154/154 [18:33<00:00,  7.23s/it, gpu_mem=0.84 GB, loss=1.84]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Avg Loss: 1.9610\n",
      "✅ Model Training Complete on GPU!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.cuda\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Track progress with tqdm\n",
    "    loop = tqdm(enumerate(train_loader), total=len(train_loader), leave=True)\n",
    "\n",
    "    for batch_idx, (images, labels) in loop:\n",
    "        images, labels = images.to(device), labels.to(device)  # Move to GPU\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Update tqdm progress bar with loss & GPU memory\n",
    "        loop.set_description(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "        loop.set_postfix(loss=loss.item(), gpu_mem=f\"{torch.cuda.memory_allocated(device) / 1e9:.2f} GB\")\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Avg Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "print(\"✅ Model Training Complete on GPU!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model saved as Trained_Model!\n"
     ]
    }
   ],
   "source": [
    "# Define model save path\n",
    "model_save_path = \"Trained_Model\"\n",
    "\n",
    "# Save model state\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "print(f\"✅ Model saved as {model_save_path}!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CUDA_env",
   "language": "python",
   "name": "cuda_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
